# -*- coding: utf-8 -*-
"""Copy of ML-HW1-GermanCreditCard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YW1_Vn-61Nt8dQQmlbw952GxLD1I4ksH

# Load the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# You can find the data under https://drive.google.com/drive/folders/1e550az93U3_kfRBbVY5PZnMKYwGYmHqi?usp=sharing

import pandas as pd
import numpy as np

train_data = pd.read_csv('/content/drive/My Drive/Machine Learning/HW1/train_data.csv')
train_label = pd.read_csv('/content/drive/My Drive/Machine Learning/HW1/train_label.csv') 

test_data = pd.read_csv('/content/drive/My Drive/Machine Learning/HW1/test_data.csv')
test_label = pd.read_csv('/content/drive/My Drive/Machine Learning/HW1/test_label.csv')

# show random samples from the training data
train_data.sample(n=5, random_state=np.random.randint(0, train_data.size))

"""# Train Decision Tree with default parameters"""

from sklearn.tree import DecisionTreeClassifier

# Train decision tree using the whole training data with **entropy** criteria

decision_tree = DecisionTreeClassifier(criterion="entropy")
decision_tree = decision_tree.fit(train_data, train_label)

# Estimate the prediction of test data
test_pred = decision_tree.predict(test_data) 

# Calculate accuracy of test data
from sklearn.metrics import accuracy_score
TestAcc = accuracy_score(test_label, test_pred) 
print("Testing Accuracy = %.5f%%" % (TestAcc * 100))

"""# FineTune Decision Tree parameters

1- Spliting dataset into train and validation
"""

# Split training data to 70% training and 30% validation
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.3)

"""2- FineTune minimum sample split"""

min_samples_splits = range(2, 100)

train_results = []
val_results = []
for min_samples_split in min_samples_splits:
  
  # Fit the tree using the 70% portion of the training data
  fineTuned_decision_tree = DecisionTreeClassifier(criterion="entropy", min_samples_split=min_samples_split)
  fineTuned_decision_tree = fineTuned_decision_tree.fit(x_train, y_train)
  #x_train --> train_data; y_train --> train_label
  
  # Evaluate on Training set
  train_pred = fineTuned_decision_tree.predict(x_train)
  train_acc = accuracy_score(y_train, train_pred)
  train_results.append(train_acc)
   
  # Evaluate on Validation set
  val_pred = fineTuned_decision_tree.predict(x_val)
  val_acc = accuracy_score(y_val, val_pred)
  val_results.append(val_acc)
  
# Ploting
import matplotlib.pyplot as plt

plt.plot(min_samples_splits, train_results, 'b')
plt.plot(min_samples_splits, val_results,'r')
plt.show()

# Choose the best minimum split sample based on the plot
Best_minSampl = 50

# Train decision tree using the full training data and the best minimum split sample
fineTuned_decision_tree = DecisionTreeClassifier(criterion="entropy", min_samples_split=Best_minSampl)
fineTuned_decision_tree = fineTuned_decision_tree.fit(train_data, train_label)

# Estimate the prediction of the test data
test_pred = fineTuned_decision_tree.predict(test_data)

# Calculate accuracy of test data
TestAcc = accuracy_score(test_label, test_pred)
print("Testing Accuracy = %.5f%%" % (TestAcc * 100))

"""# Now, apply the same procedure but using KNN instead of decision tree 

# For finetuning, find the best value of K to use with this dataset.
"""

from sklearn.neighbors import KNeighborsClassifier

# initialize the values of k to be a list of odd numbers between 1 and 30
kVals = range(1, 30, 2)

# Save the accuracies of each value of kVal in [accuracies] variable
accuracies = []

# loop over values of k for the k-Nearest Neighbor classifier
for k in kVals:
  m = KNeighborsClassifier(n_neighbors=k)
  m.fit(x_train, np.ravel(y_train))
  
  score = m.score(x_val, y_val)
  print("For k = %d, validation accuracy = %.5f%%" % (k, score * 100))
  accuracies.append(score)

# Train KNN using the full training data with the best K that you found
index = np.argmax(accuracies)
print("best k = %d with %.5f%% validation accuracy" % (kVals[index], accuracies[index] * 100))
m = KNeighborsClassifier(n_neighbors=kVals[index])
m.fit(train_data, np.ravel(train_label))

# Testing
prediction = m.predict(test_data)

testAccuracy = accuracy_score(test_label, prediction)
print("Testing Accuracy = %.5f%%" % (testAccuracy * 100))